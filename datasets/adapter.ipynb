{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install contractions"],"metadata":{"id":"RiSgTR1pyMhy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings, os\n","warnings.filterwarnings('ignore') \n","project_folder = \"/content/drive/MyDrive/Colab Notebooks/AE/SE-CNN/\"\n","os.chdir(project_folder)\n","\n","import xml.etree.ElementTree as ET\n","from string import punctuation\n","import contractions\n","\n","def xml_to_iob(in_path,out_path):\n","    tree = ET.parse(in_path)\n","    root = tree.getroot()\n","    sentences = []\n","    for sentence in root.iter(\"sentence\"):\n","            text = sentence.find(\"text\")\n","            aspectTerms = sentence.findall(\"aspectTerms\")\n","            if len(aspectTerms) > 0:\n","                for aspectTerms in sentence.iter(\"aspectTerms\"):\n","                    aspects = []\n","                    for aspectTerm in aspectTerms.iter(\"aspectTerm\"):\n","                        aspects.append(aspectTerm.attrib)\n","                    sentences.append({\"text\":text.text, \"aspects\":aspects})\n","            else:\n","                sentences.append({\"text\":text.text, \"aspects\": None})\n","    out = open(out_path,\"w\", encoding=\"utf-8\")\n","    pad = 0\n","    global_aspect_count = 0\n","    for sentence in sentences:\n","        aspects = sentence[\"aspects\"] \n","        text = sentence[\"text\"]\n","        if aspects is None:\n","            pad+=1\n","            text = text.strip()\n","            words = text.split(\" \")\n","            for word in words:\n","                if word.strip() is not \"\":\n","                    out.write(word+\"\\t\"+\"O\"+\"\\n\")\n","            out.write(\"\\n\")\n","        else:\n","            pad+=1\n","            dict = {}\n","            for aspect in aspects:\n","                term = aspect[\"term\"]\n","                from_ = int(aspect[\"from\"])\n","                to_ = int(aspect[\"to\"])\n","                if term != \"NULL\" and from_ not in dict.keys():\n","                    dict[from_] = [term,from_,to_]\n","                elif from_ in dict.keys():\n","                    print(text)\n","                    print(term == dict[from_][0])\n","            keys = sorted(dict)\n","            if len(keys) > 0:\n","                dump = \"\"\n","                last_end = 0\n","                counter = 0\n","                for key in keys:\n","                        global_aspect_count += 1\n","                        vals = dict[key]\n","                        term = vals[0]\n","                        from_ = vals[1]\n","                        to_ = vals[2]\n","                        aspect_ = text[from_:to_]\n","                        temp = text[last_end:from_]\n","                        last_end = to_\n","                        if aspect_ == term:\n","                            storage = \"\"\n","                            aspect = term.split(\" \")\n","                            i = 0\n","                            for asp in aspect:\n","                                if i == 0:\n","                                    storage = storage + asp + \"\\t\" + \"B-A\" + \"\\n\"\n","                                    i+=1\n","                                else:\n","                                    storage = storage + asp + \"\\t\" + \"I-A\" + \"\\n\"\n","                                    i+=1\n","                            temp+=storage\n","                            dump+=temp\n","                            if counter == len(keys) -1:\n","                                dump+=text[to_:]\n","                            counter+=1\n","                        else:\n","                            print(aspect_)\n","                            print(term)\n","                            print(\"NO MATCH\")\n","                            counter+=1\n","                if dump!= \"\":\n","                    dump = dump.replace(\" \",\"\\t\"+\"O\"+\"\\n\")\n","                    dump+= \"\\t\"+\"O\"\n","                    out.write(dump+\"\\n\\n\")\n","            else:\n","                text = text.strip()\n","                words = text.split(\" \")\n","                for word in words:\n","                    if word.strip() is not \"\":\n","                        out.write(word + \"\\t\" + \"O\" + \"\\n\")\n","                out.write(\"\\n\")\n","    print(global_aspect_count)\n","    out.close()\n","\n","def modefication(in_mod,out_mod):\n","  f = open(in_mod,\"r\", encoding=\"utf-8\")\n","  out = open(out_mod,\"w\", encoding=\"utf-8\")\n","  for line in f:\n","      if line.strip()!=\"\":\n","          line1 = line.split(\"\\t\")\n","          punc='!\"#$%&\\'()*+-,./:;<=>?@[\\\\]^_`{|}~'\n","          line2 = ''.join(c for c in line1[0] if c not in punc)\n","          if line2.strip() == \"\":\n","              continue\n","          else:\n","              out.write(line2+\"\\t\"+line1[1])\n","              if ',' in line1[0]:\n","                out.write(','+\"\\t\"+'O\\n')\n","      else:\n","          out.write(\"\\n\")\n","  out.close()\n","\n","def xml_to_iob_15_16(in_path,out_path):\n","    tree = ET.parse(in_path)\n","    root = tree.getroot()\n","    sentences = []\n","    for Review in root.iter(\"Review\"):\n","      for sentences_ in Review.iter(\"sentences\"):\n","        for sentence in sentences_.iter(\"sentence\"):\n","            text = sentence.find(\"text\")\n","            aspectTerms = sentence.findall(\"Opinions\")\n","            if len(aspectTerms) > 0:\n","                for aspectTerms in sentence.iter(\"Opinions\"):\n","                    aspects = []\n","                    for aspectTerm in aspectTerms.iter(\"Opinion\"):\n","                        aspects.append(aspectTerm.attrib)\n","                    sentences.append({\"text\":text.text, \"aspects\":aspects})\n","            else:\n","                sentences.append({\"text\":text.text, \"aspects\": None})\n","    out = open(out_path,\"w\", encoding=\"utf-8\")\n","    pad = 0\n","    x = 0\n","    global_aspect_count = 0\n","    for sentence in sentences:\n","        aspects = sentence[\"aspects\"]\n","        text = sentence[\"text\"]\n","        if aspects is None:\n","            pad+=1\n","            text = text.strip()\n","            words = text.split(\" \")\n","            for word in words:\n","                if word.strip() is not \"\":\n","                    out.write(word+\"\\t\"+\"O\"+\"\\n\")\n","            out.write(\"\\n\")\n","        else:\n","            pad+=1\n","            dict = {}\n","            for aspect in aspects:\n","                term = aspect[\"target\"]\n","                from_ = int(aspect[\"from\"])\n","                to_ = int(aspect[\"to\"])\n","                if term != \"NULL\" and from_ not in dict.keys():\n","                    dict[from_] = [term,from_,to_]\n","                elif from_ in dict.keys():                  \n","                    #print(text)\n","                    #print(term == dict[from_][0])\n","                    pass\n","            keys = sorted(dict)\n","            if len(keys) > 0:\n","                dump = \"\"\n","                last_end = 0\n","                counter = 0\n","                for key in keys:\n","                        global_aspect_count += 1\n","                        vals = dict[key]\n","                        term = vals[0]\n","                        from_ = vals[1]\n","                        to_ = vals[2]\n","                        aspect_ = text[from_:to_]\n","                        temp = text[last_end:from_]\n","                        last_end = to_\n","                        if aspect_ == term:\n","                            storage = \"\"\n","                            aspect = term.split(\" \")\n","                            i = 0\n","                            for asp in aspect:\n","                                if i == 0:\n","                                    storage = storage + asp + \"\\t\" + \"B-A\" + \"\\n\"\n","                                    i+=1\n","                                else:\n","                                    storage = storage + asp + \"\\t\" + \"I-A\" + \"\\n\"\n","                                    i+=1\n","                            temp+=storage\n","                            dump+=temp\n","                            if counter == len(keys) -1:\n","                                dump+=text[to_:]\n","                            counter+=1\n","                        else:\n","                            print(aspect_)\n","                            print(term)\n","                            print(\"NO MATCH\")\n","                            counter+=1\n","                if dump!= \"\":\n","                    dump = dump.replace(\" \",\"\\t\"+\"O\"+\"\\n\")\n","                    dump+= \"\\t\"+\"O\"\n","                    out.write(dump+\"\\n\\n\")\n","            else:\n","                text = text.strip()\n","                words = text.split(\" \")\n","                for word in words:\n","                    if word.strip() is not \"\":\n","                        out.write(word + \"\\t\" + \"O\" + \"\\n\")\n","                out.write(\"\\n\")\n","    print('global_aspect_count: ',global_aspect_count)\n","    out.close()\n","\n","def get_examples(input_file):\n","    \"\"\"Reads a BIO data.\"\"\"\n","    with open(input_file) as f:\n","        lines = []\n","        words = []\n","        labels = []\n","        for  line in f:\n","            contends = line.strip()\n","            word = line.strip().split('\\t')[0]\n","            label = line.strip().split('\\t')[-1]\n","            \n","            if contends.startswith(\"-DOCSTART-\"):\n","                words.append('')\n","                continue\n","            if len(contends) == 0:\n","                l = ' '.join([label for label in labels if len(label) > 0])\n","                w = ' '.join([word for word in words if len(word) > 0])\n","                lines.extend([w, l])\n","                words = []\n","                labels = []\n","                continue\n","            words.append(word)\n","            labels.append(label)\n","            \n","        return lines\n","\n","def conll03_raw_data_to_stand(root=None):\n","    for file_type in [\"train\", \"test\"]:\n","        path = root +file_type\n","        seq_in = get_examples(path + \".txt.iob\")\n","        with open(os.path.join(path, \"seq.in\"), \"w\") as seq_in_f:\n","              for seq in seq_in:\n","                  seq_in_f.write(seq + \"\\n\")\n","\n","if __name__==\"__main__\":\n","    ############### Re-14 #################\n","    in_test='datasets/Re14_Test.xml'\n","    in_train='datasets/Re14_Train.xml'\n","\n","    out_test='datasets/Re14/test.txt'\n","    out_train='datasets/Re14/train.txt'\n","\n","    xml_to_iob(in_test,out_test)\n","    xml_to_iob(in_train,out_train)\n","    modefication(out_test, out_test + '.iob')\n","    modefication(out_train, out_train + '.iob')\n","\n","    ############### La-14 #################\n","    in_test='datasets/La14_Test.xml'\n","    in_train='datasets/La14_Train.xml'\n","\n","    out_test='datasets/La14/test.txt'\n","    out_train='datasets/La14/train.txt'\n","\n","    xml_to_iob(in_test,out_test)\n","    xml_to_iob(in_train,out_train)\n","    modefication(out_test, out_test + '.iob')\n","    modefication(out_train, out_train + '.iob')\n","\n","    ############### Re-16 #################\n","    in_test='datasets/Re16_Test.xml'\n","    in_train='datasets/Re16_Train.xml'\n","\n","    out_test='datasets/Re16/test.txt'\n","    out_train='datasets/Re16/train.txt'\n","\n","    xml_to_iob_15_16(in_test,out_test)\n","    xml_to_iob_15_16(in_train,out_train)\n","    modefication(out_test, out_test + '.iob')\n","    modefication(out_train, out_train + '.iob')\n","\n","    ############### Re-15 #################\n","    in_test='datasets/Re15_Test.xml'\n","    in_train='datasets/Re15_Train.xml'\n","\n","    out_test='datasets/Re15/test.txt'\n","    out_train='datasets/Re15/train.txt'\n","\n","    xml_to_iob_15_16(in_train,out_train)\n","    xml_to_iob_15_16(in_test,out_test)\n","    modefication(out_test, out_test + '.iob')\n","    modefication(out_train, out_train + '.iob')\n","\n","    ########################################\n","    files=['datasets/Re14/', 'datasets/La14/', 'datasets/Re15/', 'datasets/Re16/']\n","    for f in files:\n","      root= project_folder+f\n","      conll03_raw_data_to_stand(root)"],"metadata":{"id":"NUabzsT8BuVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QiYWnQVxyPkq","executionInfo":{"status":"ok","timestamp":1668772920348,"user_tz":-120,"elapsed":33812,"user":{"displayName":"Ali Ahmad","userId":"13811069726907177622"}},"outputId":"58de0047-767a-4df2-8cca-b4dd9070cedd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import contractions\n","text = '''I'll pizza's. ali's car'''\n","\n","expanded_words = []\n","for word in text.split():\n","  expanded_words.append(contractions.fix(word))\n","\n","expanded_text = ' '.join(expanded_words)\n","print('Original text: ' + text)\n","print('Expanded_text: ' + expanded_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HgBIzwapjxac","executionInfo":{"status":"ok","timestamp":1668772008031,"user_tz":-120,"elapsed":12,"user":{"displayName":"Ali Ahmad","userId":"13811069726907177622"}},"outputId":"3035ad70-7cef-4fc7-be17-cb46f5113b9d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text: I'll pizza's. ali's car\n","Expanded_text: I will pizza's. ali's car\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5q1QGD7Oll6I","executionInfo":{"status":"ok","timestamp":1668772481180,"user_tz":-120,"elapsed":13,"user":{"displayName":"Ali Ahmad","userId":"13811069726907177622"}},"outputId":"9f7ce804-6676-4d2d-b109-a2fa8678dabf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"]}]},{"cell_type":"code","source":["from transformers import RobertaTokenizer\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"metadata":{"id":"QvHN445-lmJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.tokenize(\"don't pizza's\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wu6o4AqwmVWm","executionInfo":{"status":"ok","timestamp":1668772673946,"user_tz":-120,"elapsed":412,"user":{"displayName":"Ali Ahmad","userId":"13811069726907177622"}},"outputId":"565f529d-c7e2-447e-d5ef-cb03d3a99b46"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['don', 'Ġpizza', \"Ġ'\", 's']"]},"metadata":{},"execution_count":8}]}]}