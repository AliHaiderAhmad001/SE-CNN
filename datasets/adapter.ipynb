{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import warnings, os\n","warnings.filterwarnings('ignore') \n","project_folder = \"/content/drive/MyDrive/Colab Notebooks/AE/SE-CNN/\"\n","os.chdir(project_folder)\n","\n","import xml.etree.ElementTree as ET\n","from string import punctuation\n","\n","def xml_to_iob(in_path,out_path):\n","    tree = ET.parse(in_path)\n","    root = tree.getroot()\n","    sentences = []\n","    for sentence in root.iter(\"sentence\"):\n","            text = sentence.find(\"text\")\n","            aspectTerms = sentence.findall(\"aspectTerms\")\n","            if len(aspectTerms) > 0:\n","                for aspectTerms in sentence.iter(\"aspectTerms\"):\n","                    aspects = []\n","                    for aspectTerm in aspectTerms.iter(\"aspectTerm\"):\n","                        aspects.append(aspectTerm.attrib)\n","                    sentences.append({\"text\":text.text, \"aspects\":aspects})\n","            else:\n","                sentences.append({\"text\":text.text, \"aspects\": None})\n","    out = open(out_path,\"w\", encoding=\"utf-8\")\n","    pad = 0\n","    global_aspect_count = 0\n","    for sentence in sentences:\n","        aspects = sentence[\"aspects\"] \n","        text = sentence[\"text\"]\n","        if aspects is None:\n","            pad+=1\n","            text = text.strip()\n","            words = text.split(\" \")\n","            for word in words:\n","                if word.strip() is not \"\":\n","                    out.write(word+\"\\t\"+\"O\"+\"\\n\")\n","            out.write(\"\\n\")\n","        else:\n","            pad+=1\n","            dict = {}\n","            for aspect in aspects:\n","                term = aspect[\"term\"]\n","                from_ = int(aspect[\"from\"])\n","                to_ = int(aspect[\"to\"])\n","                if term != \"NULL\" and from_ not in dict.keys():\n","                    dict[from_] = [term,from_,to_]\n","                elif from_ in dict.keys():\n","                    print(text)\n","                    print(term == dict[from_][0])\n","            keys = sorted(dict)\n","            if len(keys) > 0:\n","                dump = \"\"\n","                last_end = 0\n","                counter = 0\n","                for key in keys:\n","                        global_aspect_count += 1\n","                        vals = dict[key]\n","                        term = vals[0]\n","                        from_ = vals[1]\n","                        to_ = vals[2]\n","                        aspect_ = text[from_:to_]\n","                        temp = text[last_end:from_]\n","                        last_end = to_\n","                        if aspect_ == term:\n","                            storage = \"\"\n","                            aspect = term.split(\" \")\n","                            i = 0\n","                            for asp in aspect:\n","                                if i == 0:\n","                                    storage = storage + asp + \"\\t\" + \"B-A\" + \"\\n\"\n","                                    i+=1\n","                                else:\n","                                    storage = storage + asp + \"\\t\" + \"I-A\" + \"\\n\"\n","                                    i+=1\n","                            temp+=storage\n","                            dump+=temp\n","                            if counter == len(keys) -1:\n","                                dump+=text[to_:]\n","                            counter+=1\n","                        else:\n","                            print(aspect_)\n","                            print(term)\n","                            print(\"NO MATCH\")\n","                            counter+=1\n","                if dump!= \"\":\n","                    dump = dump.replace(\" \",\"\\t\"+\"O\"+\"\\n\")\n","                    dump+= \"\\t\"+\"O\"\n","                    out.write(dump+\"\\n\\n\")\n","            else:\n","                text = text.strip()\n","                words = text.split(\" \")\n","                for word in words:\n","                    if word.strip() is not \"\":\n","                        out.write(word + \"\\t\" + \"O\" + \"\\n\")\n","                out.write(\"\\n\")\n","    print(global_aspect_count)\n","    out.close()\n","\n","def modefication(in_mod,out_mod):\n","  f = open(in_mod,\"r\", encoding=\"utf-8\")\n","  out = open(out_mod,\"w\", encoding=\"utf-8\")\n","  for line in f:\n","      if line.strip()!=\"\":\n","          line1 = line.split(\"\\t\")\n","          punc='!\"#$%&\\'()*+-,./:;<=>?@[\\\\]^_`’‘{|}~'\n","          line2 = ''.join(c for c in line1[0] if c not in punc).replace('…','')\n","          if line2.strip() == \"\":\n","              continue\n","          else:\n","              out.write(line2+\"\\t\"+line1[1])\n","              if ',' in line1[0]:\n","                out.write(','+\"\\t\"+'O\\n')\n","      else:\n","          out.write(\"\\n\")\n","  out.close()\n","\n","def xml_to_iob_15_16(in_path,out_path):\n","    tree = ET.parse(in_path)\n","    root = tree.getroot()\n","    sentences = []\n","    for Review in root.iter(\"Review\"):\n","      for sentences_ in Review.iter(\"sentences\"):\n","        for sentence in sentences_.iter(\"sentence\"):\n","            text = sentence.find(\"text\")\n","            aspectTerms = sentence.findall(\"Opinions\")\n","            if len(aspectTerms) > 0:\n","                for aspectTerms in sentence.iter(\"Opinions\"):\n","                    aspects = []\n","                    for aspectTerm in aspectTerms.iter(\"Opinion\"):\n","                        aspects.append(aspectTerm.attrib)\n","                    sentences.append({\"text\":text.text, \"aspects\":aspects})\n","            else:\n","                sentences.append({\"text\":text.text, \"aspects\": None})\n","    out = open(out_path,\"w\", encoding=\"utf-8\")\n","    pad = 0\n","    x = 0\n","    global_aspect_count = 0\n","    for sentence in sentences:\n","        aspects = sentence[\"aspects\"]\n","        text = sentence[\"text\"]\n","        if aspects is None:\n","            pad+=1\n","            text = text.strip()\n","            words = text.split(\" \")\n","            for word in words:\n","                if word.strip() is not \"\":\n","                    out.write(word+\"\\t\"+\"O\"+\"\\n\")\n","            out.write(\"\\n\")\n","        else:\n","            pad+=1\n","            dict = {}\n","            for aspect in aspects:\n","                term = aspect[\"target\"]\n","                from_ = int(aspect[\"from\"])\n","                to_ = int(aspect[\"to\"])\n","                if term != \"NULL\" and from_ not in dict.keys():\n","                    dict[from_] = [term,from_,to_]\n","                elif from_ in dict.keys():                  \n","                    #print(text)\n","                    #print(term == dict[from_][0])\n","                    pass\n","            keys = sorted(dict)\n","            if len(keys) > 0:\n","                dump = \"\"\n","                last_end = 0\n","                counter = 0\n","                for key in keys:\n","                        global_aspect_count += 1\n","                        vals = dict[key]\n","                        term = vals[0]\n","                        from_ = vals[1]\n","                        to_ = vals[2]\n","                        aspect_ = text[from_:to_]\n","                        temp = text[last_end:from_]\n","                        last_end = to_\n","                        if aspect_ == term:\n","                            storage = \"\"\n","                            aspect = term.split(\" \")\n","                            i = 0\n","                            for asp in aspect:\n","                                if i == 0:\n","                                    storage = storage + asp + \"\\t\" + \"B-A\" + \"\\n\"\n","                                    i+=1\n","                                else:\n","                                    storage = storage + asp + \"\\t\" + \"I-A\" + \"\\n\"\n","                                    i+=1\n","                            temp+=storage\n","                            dump+=temp\n","                            if counter == len(keys) -1:\n","                                dump+=text[to_:]\n","                            counter+=1\n","                        else:\n","                            print(aspect_)\n","                            print(term)\n","                            print(\"NO MATCH\")\n","                            counter+=1\n","                if dump!= \"\":\n","                    dump = dump.replace(\" \",\"\\t\"+\"O\"+\"\\n\")\n","                    dump+= \"\\t\"+\"O\"\n","                    out.write(dump+\"\\n\\n\")\n","            else:\n","                text = text.strip()\n","                words = text.split(\" \")\n","                for word in words:\n","                    if word.strip() is not \"\":\n","                        out.write(word + \"\\t\" + \"O\" + \"\\n\")\n","                out.write(\"\\n\")\n","    print('global_aspect_count: ',global_aspect_count)\n","    out.close()\n","\n","def get_examples(input_file):\n","    \"\"\"Reads a BIO data.\"\"\"\n","    with open(input_file) as f:\n","        lines = []\n","        words = []\n","        labels = []\n","        for  line in f:\n","            contends = line.strip()\n","            word = line.strip().split('\\t')[0]\n","            label = line.strip().split('\\t')[-1]\n","            \n","            if contends.startswith(\"-DOCSTART-\"):\n","                words.append('')\n","                continue\n","            if len(contends) == 0:\n","                l = ' '.join([label for label in labels if len(label) > 0])\n","                w = ' '.join([word for word in words if len(word) > 0])\n","                lines.extend([w, l])\n","                words = []\n","                labels = []\n","                continue\n","            words.append(word)\n","            labels.append(label)\n","            \n","        return lines\n","\n","def conll03_raw_data_to_stand(root=None):\n","    for file_type in [\"train\", \"test\"]:\n","        path = root +file_type\n","        seq_in = get_examples(path + \".txt.iob\")\n","        with open(os.path.join(path, \"seq.in\"), \"w\") as seq_in_f:\n","              for seq in seq_in:\n","                  seq_in_f.write(seq + \"\\n\")\n","\n","if __name__==\"__main__\":\n","    ############### Re-14 #################\n","    in_test='datasets/Re14_Test.xml'\n","    in_train='datasets/Re14_Train.xml'\n","\n","    out_test='datasets/Re14/test.txt'\n","    out_train='datasets/Re14/train.txt'\n","\n","    xml_to_iob(in_test,out_test)\n","    xml_to_iob(in_train,out_train)\n","    modefication(out_test, out_test + '.iob')\n","    modefication(out_train, out_train + '.iob')\n","\n","    ############### La-14 #################\n","    in_test='datasets/La14_Test.xml'\n","    in_train='datasets/La14_Train.xml'\n","\n","    out_test='datasets/La14/test.txt'\n","    out_train='datasets/La14/train.txt'\n","\n","    xml_to_iob(in_test,out_test)\n","    xml_to_iob(in_train,out_train)\n","    modefication(out_test, out_test + '.iob')\n","    modefication(out_train, out_train + '.iob')\n","\n","    ############### Re-16 #################\n","    in_test='datasets/Re16_Test.xml'\n","    in_train='datasets/Re16_Train.xml'\n","\n","    out_test='datasets/Re16/test.txt'\n","    out_train='datasets/Re16/train.txt'\n","\n","    xml_to_iob_15_16(in_test,out_test)\n","    xml_to_iob_15_16(in_train,out_train)\n","    modefication(out_test, out_test + '.iob')\n","    modefication(out_train, out_train + '.iob')\n","\n","    ############### Re-15 #################\n","    in_test='datasets/Re15_Test.xml'\n","    in_train='datasets/Re15_Train.xml'\n","\n","    out_test='datasets/Re15/test.txt'\n","    out_train='datasets/Re15/train.txt'\n","\n","    xml_to_iob_15_16(in_train,out_train)\n","    xml_to_iob_15_16(in_test,out_test)\n","    modefication(out_test, out_test + '.iob')\n","    modefication(out_train, out_train + '.iob')\n","\n","    ########################################\n","    files=['datasets/Re14/', 'datasets/La14/', 'datasets/Re15/', 'datasets/Re16/']\n","    for f in files:\n","      root= project_folder+f\n","      conll03_raw_data_to_stand(root)"],"metadata":{"id":"NUabzsT8BuVn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668891465695,"user_tz":-120,"elapsed":1702,"user":{"displayName":"Ramez Abdullah","userId":"12404125189717460653"}},"outputId":"d564fa73-bbdb-4ad5-95e3-fdad66a12639"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["1134\n","3693\n","654\n","2358\n","global_aspect_count:  612\n","global_aspect_count:  1743\n","global_aspect_count:  1199\n","global_aspect_count:  542\n"]}]},{"cell_type":"code","source":["line = \"crust is thin…keep that\".replace('…',' ')\n","\n","line1 = line.split(\"\\t\")\n","punc='!\"#$%&\\'()*+-,./:;<=>?@[\\\\]^_`’‘…{|}~'\n","line2 = ''.join(c for c in line1[0] if c not in punc)\n"],"metadata":{"id":"Hi4WUDbnlIFS","executionInfo":{"status":"ok","timestamp":1668891331960,"user_tz":-120,"elapsed":510,"user":{"displayName":"Ramez Abdullah","userId":"12404125189717460653"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["line2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"FMMKLHB2rNcD","executionInfo":{"status":"ok","timestamp":1668891336202,"user_tz":-120,"elapsed":800,"user":{"displayName":"Ramez Abdullah","userId":"12404125189717460653"}},"outputId":"a09acd8b-a292-48be-a266-b63db97ebbdb"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'crust is thin keep that'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]}]}